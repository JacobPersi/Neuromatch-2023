{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (800, 360, 25)\n",
      "Y: (800,)\n"
     ]
    }
   ],
   "source": [
    "%run \"./1. Data Loading.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/renqi1/Time_Series_Classification/blob/main/Rocket.py\n",
    "# paper: https://arxiv.org/abs/1910.13051\n",
    "# code: https://github.com/angus924/rocket\n",
    "\n",
    "class Rocket(nn.Module):\n",
    "    def __init__(self, n_features, seq_len, n_classes, n_kernels=100, kss=[7, 9, 11]):\n",
    "        super(Rocket, self).__init__()\n",
    "        kss = [ks for ks in kss if ks < seq_len]\n",
    "        convs = nn.ModuleList()\n",
    "        for i in range(n_kernels):\n",
    "            ks = np.random.choice(kss)\n",
    "            dilation = 2**np.random.uniform(0, np.log2((seq_len - 1) // (ks - 1)))\n",
    "            padding = int((ks - 1) * dilation // 2) if np.random.randint(2) == 1 else 0\n",
    "            weight = torch.randn(1, n_features, ks)\n",
    "            weight -= weight.mean()\n",
    "            bias = 2 * (torch.rand(1) - .5)\n",
    "            layer = nn.Conv1d(n_features, 1, ks, padding=2 * padding, dilation=int(dilation), bias=True)\n",
    "            layer.weight = torch.nn.Parameter(weight, requires_grad=False)\n",
    "            layer.bias = torch.nn.Parameter(bias, requires_grad=False)\n",
    "            convs.append(layer)\n",
    "        self.convs = convs\n",
    "        self.n_kernels = n_kernels\n",
    "        self.feature_dim = 2 * n_kernels\n",
    "        self.fc = nn.Linear(self.feature_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input x: [B, F, T],  where B = Batch size, F = features, T = Time sampels,\n",
    "        _output = []\n",
    "        for i in range(self.n_kernels):\n",
    "            out = self.convs[i](x).cpu()\n",
    "            _max = out.max(dim=-1)[0]\n",
    "            _ppv = torch.gt(out, 0).sum(dim=-1).float() / out.shape[-1]\n",
    "            _output.append(_max)\n",
    "            _output.append(_ppv)\n",
    "        output = torch.cat(_output, dim=1)#.cuda()      # [batch_size, feature_dim]\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 709.0549\n",
      "Epoch [1/10], Accuracy: 0.6500\n",
      "Epoch [2/10], Loss: 278.7373\n",
      "Epoch [2/10], Accuracy: 0.7000\n",
      "Epoch [3/10], Loss: 222.3217\n",
      "Epoch [3/10], Accuracy: 0.7562\n",
      "Epoch [4/10], Loss: 179.5808\n",
      "Epoch [4/10], Accuracy: 0.8063\n",
      "Epoch [5/10], Loss: 151.0640\n",
      "Epoch [5/10], Accuracy: 0.8250\n",
      "Epoch [6/10], Loss: 132.5798\n",
      "Epoch [6/10], Accuracy: 0.8625\n",
      "Epoch [7/10], Loss: 114.9918\n",
      "Epoch [7/10], Accuracy: 0.8688\n",
      "Epoch [8/10], Loss: 100.3237\n",
      "Epoch [8/10], Accuracy: 0.8812\n",
      "Epoch [9/10], Loss: 91.5915\n",
      "Epoch [9/10], Accuracy: 0.8875\n",
      "Epoch [10/10], Loss: 79.0048\n",
      "Epoch [10/10], Accuracy: 0.8938\n"
     ]
    }
   ],
   "source": [
    "combined = list(zip(X, Y))\n",
    "random.shuffle(combined)\n",
    "shuffled_x_values, shuffled_y_values = zip(*combined)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(shuffled_x_values, shuffled_y_values, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define hyperparameters\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "n_features = 360\n",
    "seq_len = 25\n",
    "n_classes = 25\n",
    "n_kernels = 100\n",
    "kss = [3, 5, 7, 9, 11]\n",
    "\n",
    "# Create the Rocket model\n",
    "model = Rocket(n_features, seq_len, n_classes, n_kernels, kss)\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_train_samples = len(X_train)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    # Mini-batch training\n",
    "    for batch_start in range(0, num_train_samples, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, num_train_samples)\n",
    "        inputs = torch.tensor(X_train[batch_start:batch_end], dtype=torch.float32)#.to(device)\n",
    "        labels = torch.tensor(Y_train[batch_start:batch_end], dtype=torch.long)#.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() * (batch_end - batch_start)\n",
    "    \n",
    "    # Compute average epoch loss\n",
    "    epoch_loss /= num_train_samples\n",
    "    \n",
    "    # Print training loss for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_start in range(0, len(X_test), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, len(X_test))\n",
    "            inputs = torch.tensor(X_test[batch_start:batch_end], dtype=torch.float32)#.to(device)\n",
    "            labels = torch.tensor(Y_test[batch_start:batch_end], dtype=torch.long)#.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute predictions and accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            num_correct += (predicted == labels).sum().item()\n",
    "            num_total += labels.size(0)\n",
    "    \n",
    "    # Compute and print evaluation metrics\n",
    "    accuracy = num_correct / num_total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save the trained model if desired\n",
    "#torch.save(model.state_dict(), \"rocket_model.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
